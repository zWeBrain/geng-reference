{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import heapq\n",
    "import shutil\n",
    "import queue as Q\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import nltk\n",
    "from model import TrieNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.exists('blocks')\n",
    "if not folder:\n",
    "    os.makedirs('blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = '动漫'\n",
    "files= os.listdir(ori_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct1(intToJuji, intToFanju):\n",
    "    i = 0\n",
    "    j = -1000\n",
    "    k = 0\n",
    "    terms_id = dict()\n",
    "    for root, dirs, files in os.walk(ori_path, topdown=True):\n",
    "        if '\\.ipynb_checkpoints' in root:\n",
    "            continue\n",
    "        if len(files) == 0:\n",
    "            j+=1000\n",
    "            i=0\n",
    "            intToFanju[j] = root\n",
    "        else:\n",
    "            i+=1\n",
    "            key = j+i\n",
    "            intToJuji[key] = root\n",
    "            for name in files:\n",
    "                f = open(root+\"\\\\\"+ name, errors='ignore')\n",
    "                lines = f.readlines()\n",
    "                texts_s,texts_l = arrayTodict(lines)\n",
    "                counts = cut3(texts_s)\n",
    "            for word in counts:\n",
    "                terms_id[word] = terms_id.get(word, 0) + counts[word]\n",
    "    return terms_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct(intToJuji, intToFanju):\n",
    "    i = 0\n",
    "    j = -1000\n",
    "    k = 0\n",
    "    terms_id = dict()\n",
    "    doc_len = dict()\n",
    "    for root, dirs, files in os.walk(ori_path, topdown=True):\n",
    "        if '\\.ipynb_checkpoints' in root:\n",
    "            continue\n",
    "        if len(files) == 0:\n",
    "            if j>0:\n",
    "                build_c(j, terms_id)\n",
    "                terms_id = dict()\n",
    "            j+=1000\n",
    "            i=0\n",
    "            intToFanju[j] = root\n",
    "        else:\n",
    "            i+=1\n",
    "            key = j+i\n",
    "            intToJuji[key] = root\n",
    "            counts = dict()\n",
    "            for name in files:\n",
    "                f = open(root+\"\\\\\"+ name, errors='ignore')\n",
    "                lines = f.readlines()\n",
    "                texts_l = arrayTodict(lines)\n",
    "                counts = dict_union(counts, cut1(texts_l))\n",
    "            for word in counts:\n",
    "                if word.isalpha() and counts[word]<3:\n",
    "                    continue\n",
    "                if key in doc_len:\n",
    "                    doc_len[key] += counts[word]\n",
    "                else:\n",
    "                    doc_len[key] = counts[word]\n",
    "                if word in terms_id:\n",
    "                    terms_id[word].append((key,counts[word]))\n",
    "                else: \n",
    "                    terms_id[word] = [(key,counts[word])]\n",
    "    build_c(j, terms_id)\n",
    "    terms_id = dict()\n",
    "    return doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intToJuji = dict()\n",
    "intToFanju= dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30306"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intToJuji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = construct(intToJuji, intToFanju)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27050"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = sorted(doc_len.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    path = \"doclen_new.txt\"\n",
    "    with open(path, 'w', newline='') as f:\n",
    "        for key in doc_len:\n",
    "            f.write(str(key))\n",
    "            f.write('-')\n",
    "            f.write(str(doc_len[key]))\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_words = sorted(small_words.items(),key = lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayTodict(lines):\n",
    "    word_dict_s={}\n",
    "    word_dict_l={}\n",
    "    for line in lines:\n",
    "        line=shorten(line.strip('\\n').replace(' ', ''))\n",
    "        if len(line)<2:\n",
    "            continue\n",
    "        if len(line)<5:\n",
    "            if word_dict_s.get(line):\n",
    "                word_dict_s[line] += 1\n",
    "            else:\n",
    "                word_dict_s[line] = 1\n",
    "            continue\n",
    "        if word_dict_l.get(line):\n",
    "            word_dict_l[line] += 1\n",
    "        else:\n",
    "            word_dict_l[line] = 1\n",
    "    return word_dict_s, word_dict_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayTodict(lines):\n",
    "    word_dict_l={}\n",
    "    for line in lines:\n",
    "        line=shorten(line.strip('\\n').replace(' ', ''))\n",
    "        if len(line)<2 or bool(re.search(r'\\d', line)):\n",
    "            continue\n",
    "        if word_dict_l.get(line):\n",
    "            word_dict_l[line] += 1\n",
    "        else:\n",
    "            word_dict_l[line] = 1\n",
    "    return word_dict_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_union(d1, d2):\n",
    "    keys = d1.keys() | d2.keys()\n",
    "    temp = {}\n",
    "    for key in keys:\n",
    "        temp[key] = sum([d.get(key,0) for d in (d1, d2)])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut1(word_dict):\n",
    "    counts = dict()\n",
    "    for line in word_dict:\n",
    "        num = word_dict[line]\n",
    "        words = jieba.lcut(line)\n",
    "        if len(words) > 5:\n",
    "            continue\n",
    "        for word in words:\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                counts[word] = counts.get(word, 0) + num\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut3(word_dict):\n",
    "    counts = dict()\n",
    "    for line in word_dict:\n",
    "        num = word_dict[line]\n",
    "        counts[line] = counts.get(line, 0) + num\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut2(word_dict):\n",
    "    counts = dict()\n",
    "    for line in word_dict:\n",
    "        num = word_dict[line]\n",
    "        if len(line)<5:\n",
    "            counts[line] = counts.get(line, 0) + num\n",
    "            continue\n",
    "        words = jieba.lcut(line)\n",
    "        for word in words:\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                counts[word] = counts.get(word, 0) + num\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_c(i, terms_id):\n",
    "    list1 = []\n",
    "    path = \"chunks_big/chunk_\"+str(i)+\".txt\"\n",
    "    list1 = sorted(terms_id.items(), key=lambda item: item[0])\n",
    "    with open(path, 'w', newline='') as f:\n",
    "        for key in list1:\n",
    "            list2= []\n",
    "            f.write(str(key[0]))\n",
    "            for word in key[1]:\n",
    "                f.write(',')\n",
    "                f.write(str(word[0]))\n",
    "                f.write('-')\n",
    "                f.write(str(word[1]))\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs():\n",
    "    # Create a new folder to generate inverted indexes\n",
    "    folder1 = os.path.exists('chunks')\n",
    "    folder2 = os.path.exists('blocks')\n",
    "    if not folder1:\n",
    "        os.makedirs('chunks')\n",
    "    if not folder2:\n",
    "        os.makedirs('blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'233'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten('2333')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'233'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten('23333333')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateHeader(nodeToTest, targetNode):\n",
    "    while nodeToTest.nodeLink != None:\n",
    "        nodeToTest = nodeToTest.nodeLink\n",
    "    nodeToTest.nodeLink = targetNode\n",
    "def updateFPtree(items, inTree, headerTable, count):\n",
    "    if items[0] in inTree.children:\n",
    "        # 判断items的第一个结点是否已作为子结点\n",
    "        inTree.children[items[0]].inc(count)\n",
    "    else:\n",
    "        # 创建新的分支\n",
    "        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
    "        # 更新相应频繁项集的链表，往后添加\n",
    "        if headerTable[items[0]][1] == None:\n",
    "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
    "        else:\n",
    "            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
    "    # 递归\n",
    "    if len(items) > 1:\n",
    "        updateFPtree(items[1::], inTree.children[items[0]], headerTable, count)\n",
    "\n",
    "def createFPtree(word_dict, cutted, minSup=1):\n",
    "    headerTable = {}\n",
    "    for trans in word_dict:\n",
    "        for item in trans:\n",
    "            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
    "    freqItemSet = set(headerTable.keys()) # 满足最小支持度的频繁项集\n",
    "    if len(freqItemSet) == 0:\n",
    "        return None, None\n",
    "    for k in headerTable:\n",
    "        headerTable[k] = [headerTable[k], None] # element: [count, node]\n",
    "\n",
    "    retTree = treeNode('Null Set', 1, None)\n",
    "    for tranSet, count in dataSet.items():\n",
    "        # dataSet：[element, count]\n",
    "        localD = {}\n",
    "        for item in tranSet:\n",
    "            if item in freqItemSet: # 过滤，只取该样本中满足最小支持度的频繁项\n",
    "                localD[item] = headerTable[item][0] # element : count\n",
    "        if len(localD) > 0:\n",
    "            # 根据全局频数从大到小对单样本排序\n",
    "            orderedItem = [v[0] for v in localD.items()]\n",
    "            # 用过滤且排序后的样本更新树\n",
    "            updateFPtree(orderedItem, retTree, headerTable, count)\n",
    "    return retTree, headerTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吉良吉影/觉得很赞\n"
     ]
    }
   ],
   "source": [
    "print (\"/\".join(jieba.cut(\"吉良吉影觉得很赞\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = []\n",
    "f = open(\"new_words_all.txt\")\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    split = line.strip().split(',')\n",
    "    new_words.append((split[0],int(split[1])))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('233', 251716)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_word in new_words:\n",
    "    if key_word[1] > 2:\n",
    "        jieba.add_word(key_word[0], freq = key_word[1]*10, tag = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(intToJuji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30306"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('juji.txt','w', encoding=\"utf-8\")\n",
    "for juji in intToJuji:\n",
    "    f.write(str(juji))\n",
    "    f.write(',')\n",
    "    f.write(intToJuji[juji])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_blocks_heap(N):\n",
    "    # SPIMI\n",
    "    reader = txt_reader('chunks_big')\n",
    "    heap = []\n",
    "    for i in range(len(reader)):\n",
    "        next_line = reader[i].readline().strip().split(',')\n",
    "        heapq.heappush(heap, (next_line[0], next_line[1:], i))\n",
    "\n",
    "    terms_id = dict()\n",
    "    j = 0\n",
    "    next_term = ''\n",
    "    try:\n",
    "        while not len(heap) == 0:\n",
    "            next_index = heapq.heappop(heap)\n",
    "            judge = (next_index[0] == next_term)\n",
    "            next_term = next_index[0]\n",
    "            next_line = reader[next_index[2]].readline()\n",
    "            if next_line:\n",
    "                next_line= next_line.strip().split(',')\n",
    "                heapq.heappush(heap, (next_line[0], next_line[1:], next_index[2]))\n",
    "            else:\n",
    "                reader[next_index[2]].close()\n",
    "            if (sys.getsizeof(terms_id) > 5000000 and not judge):\n",
    "                j = j + 1\n",
    "                build_b(j, terms_id, N)\n",
    "                terms_id = dict()\n",
    "            if next_term in terms_id:\n",
    "                terms_id[next_term].extend(next_index[1])\n",
    "            else: \n",
    "                terms_id[next_term] = next_index[1]\n",
    "    finally:\n",
    "        j=j+1\n",
    "        build_b(j, terms_id, N)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_b(i, terms_id, N):\n",
    "    path = \"blocks_big/block_\"+ str(i) +\".csv\"\n",
    "    with open(path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        try:\n",
    "            for key in terms_id:\n",
    "                if len(terms_id[key]) == 1:\n",
    "                    continue\n",
    "                list2= []\n",
    "                list2.append(key)\n",
    "                idf = math.log(N /(len(terms_id[key])+1), 10)\n",
    "                list2.append(idf)\n",
    "                list2.extend(terms_id[key])\n",
    "                writer.writerow(list2)\n",
    "        finally:\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_blocks_heap(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_reader(path):\n",
    "    fileList = os.listdir(path)\n",
    "    files_reader = dict()\n",
    "    i = 0\n",
    "    for file in fileList:\n",
    "        if(file.split('.')[-1] == \"txt\"):\n",
    "            name = path+'/'+file\n",
    "            files_reader[i] = open(name, \"r\")\n",
    "            i = i + 1\n",
    "    return files_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'blocks_big'\n",
    "fileList = os.listdir(path)\n",
    "files_reader = dict()\n",
    "for file in fileList:\n",
    "    if(file.split('.')[-1] == \"csv\"):\n",
    "        name = path+'/'+file\n",
    "        com = re.compile(r\"_(\\d+).csv\", re.M)\n",
    "        find = com.findall(name)\n",
    "        i = int(find[0])\n",
    "        files_reader[i] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
